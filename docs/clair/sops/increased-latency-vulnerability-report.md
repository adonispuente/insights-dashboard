# Matcher increased latency for GETting vulnerability reports

## Description:
Quay's UI will request a Vulnerability Report from a Clair matcher for every tag that appears in the tags view. High latency when responding to such requests will result in a laggy UI experience and potential overloading of the Matcher pods as more requests are in-flight.

## Observed:
- [Grafana dashboard](https://grafana.app-sre.devshift.net/d/I1JBFlRnz/clair-v4?orgId=1&var-rate=1m&var-dbquantile=0.95&var-apiquantile=0.20&var-datasource=clairp01ue1-prometheus&viewPanel=7)

## Debugging steps:
- Browse to the logs in [Cloudwatch](logs.md)
- Use the query:
```
fields @timestamp, message
| filter kubernetes.namespace_name = "clair-production"
| filter kubernetes.labels.service = "matcher"
| filter level = "error"
| sort @timestamp desc
```
- If, from the logs, the issue seems database related or there are multiple context timeouts in the logs, go to the [Grafana RDS dashboard](db_dashboards.md) and check how things look.
- If the Matcher DB has increased connections it's possible there is a connection leak in the Matcher.
- If the DB resources are saturated then it's possible requests to the DB completing slowly.
- If the logs don't seem to show any errors it is possible the matcher pods are running out of CPU and are being throttled, this can be determined through [these pod dashboards](pod_dashboards.md).
## Resolution steps:
- If it is found to be a problem with DB compute resources then it is likely a bigger RDS instance type will be needed, this should be escalated to Quay oncall as it will likely incur downtime.
- If the matcher pods are constantly hitting the defined CPU limit then the CPU limit should be increased by 500mCPU.
- If no resolution is found then scaling the matcher pods can also be done to spread the load. However, care should be taken to ensure the matcher DB is still operating in safe bounds (i.e. CPU is below 80% usage and Memory isn't becoming saturated).
- If no resolution is found, escalate Quay oncall.
