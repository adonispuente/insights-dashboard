# Matcher increase 500s for GETting vulnerability reports

## Description:
Quay's UI will request a Vulnerability Report from a Clair matcher for every tag that appears in the tags view. When the matcher experiences an unexpected error responding to this vulnerability report request it will return a 500 status code.

## Observed:
- [Grafana dashboard](https://grafana.app-sre.devshift.net/d/I1JBFlRnz/clair-v4?orgId=1&var-rate=1m&var-dbquantile=0.95&var-apiquantile=0.20&var-datasource=clairp01ue1-prometheus&viewPanel=7)

## Debugging steps:
- Browse to the logs in [Cloudwatch](logs.md)
- Use the query:
```
fields @timestamp, message
| filter kubernetes.namespace_name = "clair-production"
| filter kubernetes.labels.service = "matcher"
| filter level = "error"
| sort @timestamp desc
```
- If, from the logs, the issue seems database related or there are multiple context timeouts in the logs, go to the [Grafana RDS dashboard](db_dashboards.md) and check how things look.
- If the Matcher DB has no connections then probably there is a networking problem between Clair and RDS.
- If the DB resources are saturated then it's possible requests to the DB are timing out or failing completely.
- If the logs don't seem to show any errors it is possible the matcher pods are running out of resources and are being restarted, this can be determined through [these pod dashboards](pod_dashboards.md) and going through the debugging steps [here](pods-restarting.md).
## Resolution steps:
- If it is found to be a networking issue then steps should be taken on the infrastructure side to rectify the problem.
- If it is found to be a problem with DB compute resources then it is likely a bigger RDS instance type will be needed, this should be escalated to Quay oncall as it will likely incur downtime.
- If the indexer pods are restarting because of exhausted memory resources then memory limits and requests should be evaluated and increased, adding 500Mb of memory to the requested amount and redeploying.
- If no resolution is found then scaling the matcher pods can also be done to spread the load. However, care should be taken to ensure the matcher DB is still operating in safe bounds (i.e. CPU is below 80% usage and Memory isn't becoming saturated).
- If no resolution is found, escalate Quay oncall.
