- [Info](#info)
- [Process](#process)
  - [Provisioning the cluster](#provisioning-the-cluster)
  - [Provisioning Hive](#provisioning-hive)
  - [Hive Monitoring](#hive-monitoring)
  - [Provisioning OSD operators](#provisioning-osd-operators)
    - [Resources](#resources)
    - [Others](#others)
  - [Validations](#validations)
  - [Adding the shard to OCM](#adding-the-shard-to-ocm)
  - [OSD operators notes](#osd-operators-notes)

# Info

**Aim of this doc:** This SOP serves as a step-by-step process on how to provision a new Hive shard from zero (no cluster) to a fully functioning hive shard used by OCM

**Feeds into Goal:** Scale out OSD + Managed Services control plane to meet increasing business demand for capacity, reduce risk and scale.

**Definition of Done:** This initiative has met its delivery target when
- We have 4 hive clusters, entirely managed via a single consistent pipeline
- We have de-provisioned the OSD-3 based old-hive-prod
- We have proven, documented and monitor for all hive cluster hosted components and their engagement with upstream services

These instructions have been adapted from the [original google doc](https://docs.google.com/document/d/1oXcxKFsiNyBxUi0IizJ_VAfMe-CgtyqFLV0vNj4ib8I/edit)

# Process

## Provisioning the cluster

1. Follow the standard [Cluster Onboarding SOP](app-interface-onboard-cluster.md) using the following specs

    |                    | Staging     | Production |
    |--------------------|-------------|------------|
    | Availability       | Single zone | Multizone  |
    | Compute type       | m5.xlarge   | m5.xlarge  |
    | Compute count      | 4           | 9          |
    | Persistent storage | 600 GB      | 600 GB     |
    | Load balancers     | 0           | 0          |
    | Machine CIRD       | See note    | See note   |
    | Privacy            | Private     | Private    |

1. Configure VPC peering
1. Add hive-readers and hive-admins in `cluster.yml`
1. Add `ClusterRoleBinding` (required?)

   - Example: https://issues.redhat.com/projects/OHSS/issues/OHSS-495

## Provisioning Hive

Provisioning hive is a multi-step process:
1. Create a new environment for this hive cluster in [/data/products/osdv4/environments](/data/products/osdv4/environments) and make sure that the namespaces created from this moment on in the new cluster belong to it.
1. Add the hive namespaces. Add a new directory named after shard in [`/data/services/hive/namespaces`](/data/services/hive/namespaces) and copy the contents of another shard from the same hive environment (production, staging, etc)
1. Hive is deployed using a saas file. In order to deploy to a new shard, a new target must be added to the Hive saas file located here: [`/data/services/hive/cicd/ci-int/saas-hive.yaml`](/data/services/hive/cicd/ci-int/saas-hive.yaml)

1. Assign hive permissions in

    | Role file                            | Staging                                 | Production                               |
    |----------------------------------------|-----------------------------------------|------------------------------------------|
    | `data/teams/hive/roles/dev.yml`        | hive-admins<br>View access to namespace | hive-readers<br>View access to namespace |
    | `data/teams/hive/roles/qe.yml`         | hive-admins                             | hive-readers                             |
    | `data/teams/ocm/roles/dev.yml`         | hive-readers                            | hive-readers                             |
    | `data/teams/app-sre/roles/app-sre.yml` | hive-admins                             | hive-admins                              |

## Provisioning OSD operators

### Resources

#### operator resources (secrets, configmaps, etc)

Add a new directory named after the shard name here: [`/data/services/osd-operators/namespaces`](/data/services/osd-operators/namespaces). A few notes about this:

* It is typical to copy the content from another shard of the same environment as we are re-using the same configs and secrets for all shards. Unless instructed otherwise, start with a prod as an example as it will have a really working setup.
* Make sure that the namespaces belong to the environment created above.

#### SREP IDP SelectorSyncSets

The `OpenShift_SRE` IDP associated to every OSD is configured via a SelectorSyncSet in hive:

- `osd-google-secret` for production environments, e.g. [/data/services/osd-operators/namespaces/hivep01ue1/cluster-scope.yml](/data/services/osd-operators/namespaces/hivep01ue1/cluster-scope.yml)
- `osd-ldap-secret` for staging and integration, e.g. [/data/services/osd-operators/namespaces/hives02ue1/cluster-scope.yml](/data/services/osd-operators/namespaces/hives02ue1/cluster-scope.yml)

Make sure that the SSS deployed corresponds to your environment

#### saas deploy jobs

OSD operator are deployed using saas-file. In order to deploy to a new shard, a new target must be added to all of the OSD operators saas files located here: [`/data/services/osd-operators/cicd/saas`](/data/services/osd-operators/cicd/saas)

Example: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/8650 adds all the saas deploy files and selector syncsets mentioned below.

Take into account the following for two of the operators

##### cloud-ingress-operator

Ensure that the new hive shard's egress gateway IP is listed in
[`/data/services/osd-operators/cicd/saas/saas-cloud-ingress-operator.yaml`](/data/services/osd-operators/cicd/saas/saas-cloud-ingress-operator.yaml).
In order to get this, you have to access the hive shard cluster AWS account
through a impersonation link. There are two ways to get the impersonation link:

* OCM console: Go to https://cloud.redhat.com/openshift, then click into your
cluster and then you will get the details under "AWS infrastructure access" in
the "Access Control" section.

* `ocm` cli:

  * Get cluster id using:
      ```
      ocm list clusters
      ```
  * Get your impersonation link inspecting the output of:
      ```
      ID=xxxxx
      ocm get /api/clusters_mgmt/v1/clusters/$ID/aws_infrastructure_access_role_grants

      ```
    **Hint**: Look for your RedHat login name

Having access to the AWS console, find the egress gateway IP under:

* VPC -> NAT Gateways -> Elastic IP address

IMPORTANT:

* The "Elastic IP address" from all NAT gateways should be listed. 
* Add `/32` to each os the IPs in the saas file list. 

##### aws-account-operator

Once deployed AWS account operator, it has to be properly verified and enabled. Open a JIRA card such as https://issues.redhat.com/browse/OSD-5045 to get this done.

You can find further details of this process at the end of this document.

### Others

#### SelectorSyncSets

Deploy SelectorSyncSets to new hive cluster

Example MR: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/6823

#### ClusterImageSets

To deploy ClusterImageSets to a new cluster, add a target in the matching saas file: [`/data/services/osd-operators/cicd/saas/clusterimagesets`](/data/services/osd-operators/cicd/saas/clusterimagesets)

#### Managed-tenants

To deploy the managed-tenants SelectorSyncSets, add a target in the managed-tenants saas file: [`/data/services/app-sre/cicd/ci-int/saas-managed-tenants.yaml`](/data/services/app-sre/cicd/ci-int/saas-managed-tenants.yaml)

## Monitoring

All v4 hive shards (clusters) are monitored with their own workload prometheus, which runs in the `openshift-customer-monitoring` namespace.

1. Check that an `openshift-customer-monitoring` namespace file exists for the specific hive cluster. This is usually done as part of [onboarding any new cluster](https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/app-sre/sop/app-interface-onboard-cluster.md#step-4-observability)
1. Use the hive monitoring boilerplate to add hive specific monitoring rules and servicemonitors to the `openshift-customer-monitoring` namespace file for the specific hive cluster:

```
# ServiceMonitor
## Hive
- provider: resource-template
  type: extracurlyjinja2
  path: /services/hive/hive-controllers.servicemonitor.yaml
  variables:
    namespace: hive
- provider: resource-template
  type: extracurlyjinja2
  path: /services/osd-operators/hive-operator-generic.servicemonitor.yaml
  variables:
    operator_name: aws-account-operator
- provider: resource-template
  type: extracurlyjinja2
  path: /services/osd-operators/hive-operator-generic.servicemonitor.yaml
  variables:
    operator_name: certman-operator
- provider: resource-template
  type: extracurlyjinja2
  path: /services/osd-operators/hive-operator-generic.servicemonitor.yaml
  variables:
    operator_name: deadmanssnitch-operator
- provider: resource-template
  type: extracurlyjinja2
  path: /services/osd-operators/hive-operator-generic.servicemonitor.yaml
  variables:
    operator_name: pagerduty-operator


# PrometheusRule
## SHARDNAME
- provider: resource-template
  type: extracurlyjinja2
  path: /services/hive/hive-production-common.prometheusrules.yaml
  variables:
    shard_name: SHARDNAME
    aws_account_operator_accounts_threshold: 4950
    grafana_datasource: SHARDNAME-prometheus

```

1. Make sure you're using the correct rules depending on the environment (integration/stage/production). Pay close attention to the value for aws_account_operator_accounts_threshold, which depends on the environment.

1. Make RBAC changes and allow network traffic from `openshift-customer-monitoring` on all relevant namespaces. For example, see PR: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/10168/diffs. Note that you'll need to do the same for the `hive` namespace on the cluster

Post file creation checks:

1. Make sure SHARDNAME is replaced by the actual shard name, for example `hivep01ue1`

At this point, the monitoring is all set, and you're ready to move on to the next step


## Adding the shard to OCM

1. Add `uhc-leadership` namespace. Example: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/6829
1. Add `view` access to the `uhc-leadership` namespace to the OCM [dev role](/data/teams/ocm/roles/dev.yml).
1. Add Service Account references to hive, aws-account-opeator and gcp-project-operator namespaces from the uhc namespaces. Example: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/7655
1. Update `uhc-clusters-service` secret to add new shards. Example: https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/8951. We don't assign any region nor cloud provider, we will manually pin clusters to this shard in the validation phase.
1. The id field is set to a random uuid unique per shard (uuidgen can be used to generate one)

## Validations

When creating clusters make sure that you're logged in the correct environment. Use the option `--url` of the `ocm login` command to connect to the proper environment.

* You can follow the cluster creation using the `ocm status` command
* Make sure you delete the clusters created with the `ocm delete /api/clusters_mgmt/v1/clusters/<CLUSTER ID>` command

For all the created clusters perform the following validations:

* Make sure certificates are delivered quickly (at least 10 minutes from `Ready` status reported by `ocm`). You can inspect the console url:

    ```shell
    ID=<cluster id>
    CONSOLE=$(ocm get cluster $ID | jq -r '("console-openshift-console.apps." + .name + "." + .dns.base_domain)')
    openssl s_client -servername $CONSOLE -showcerts -connect $CONSOLE:443 2>/dev/null </dev/null | openssl x509 -text
    ```

    and make sure that it is not self signed. Alternative you can make sure that `curl` doesn't complain (but that needs that you have up to date CAs installed in your system, which is not always the case)

* Verify that all syncsetinstances report `Applied == true`. Currently the managed-velero sss takes a while, but after some minutes, all should report successfully applied. In order to do this you have to inspect the `ClusterSync` CRD in the shard project related to the cluster you have created, e.g

   ```shell
   oc get clustersync -n uhc-staging-1fjs44ifpfolii89opv6ua394b1qti0l -o yaml | grep result | sort --uniq
         result: Success
   ```

### Test provisioning an AWS cluster

In the following example we create a cluster pinning it to the shard we have added to OCM before.

```shell
ocm post /api/clusters_mgmt/v1/clusters <<EOJ
{
    "byoc": false,
    "name": "rporresm-aws01",
    "region": {
        "id": "us-east-1"
    },
    "nodes": {
        "compute": 4,
        "compute_machine_type": {
            "id": "m5.xlarge"
        }
    },
    "managed": true,
    "cloud_provider": {
        "id": "aws"
    },
    "multi_az": false,
    "load_balancer_quota": 0,
    "storage_quota": {
        "unit": "B",
        "value": 107374182400
    }
    ,
    "properties": {
        "provision_shard_id": "11015a3e-9e4d-4cf1-93d7-06fb2cf83a1c"
    }
}
EOJ
```

### Test provisioning a GCP cluster

Example

```shell
ocm post /api/clusters_mgmt/v1/clusters <<EOJ
{
    "byoc": false,
    "name": "gshereme-test3-sep10",
    "region": {
        "id": "us-east1"
    },
    "nodes": {
        "compute": 4,
        "compute_machine_type": {
            "id": "custom-4-16384"
        }
    },
    "managed": true,
    "cloud_provider": {
        "id": "gcp"
    },
    "multi_az": false,
    "load_balancer_quota": 0,
    "storage_quota": {
        "unit": "B",
        "value": 107374182400
    },
    "properties": {
        "provision_shard_id": "11015a3e-9e4d-4cf1-93d7-06fb2cf83a1c"
    }
}
EOJ
```

### Test that private clusters can be provisioned

Example:

```shell
ocm post /api/clusters_mgmt/v1/clusters <<EOJ
{
    "byoc": false,
    "name": "rporresm-aws02p",
    "region": {
        "id": "us-east-1"
    },
    "nodes": {
        "compute": 4,
        "compute_machine_type": {
            "id": "m5.xlarge"
        }
    },
    "managed": true,
    "cloud_provider": {
        "id": "aws"
    },
    "multi_az": false,
    "load_balancer_quota": 0,
    "storage_quota": {
        "unit": "B",
        "value": 107374182400
    },
    "network": {
        "machine_cidr": "10.225.0.0/16",
        "service_cidr": "172.30.0.0/16",
        "pod_cidr": "10.128.0.0/14"
    },
    "api": {
        "listening": "internal"
    },
    "properties": {
        "provision_shard_id": "11015a3e-9e4d-4cf1-93d7-06fb2cf83a1c"
    }
}
EOJ
```

Once created, there are two further validations

* Can SRE-P access them? Create a JIRA card in OHSS project to make sure that SREP can access the cluster. Example: https://issues.redhat.com/browse/OHSS-1321
* Does Hive report them as Reachable? To check this, you have to go into the cluster project in the hive shard (`uhc-<environment>-<cluster_id>`) and inspect the `ClusterDeployment` CRD.  Under the `.status.conditions` section you will find the `type` `ActiveAPIURLOverride` that should tell you that the cluster is reachable. e.g.

    ```shell
    oc get clusterdeployment rporresm-aws02p -o json | jq '.status.conditions[] | select(.type | contains("ActiveAPIURLOverride"))'
    {
      "lastProbeTime": "2020-09-10T16:20:10Z",
      "lastTransitionTime": "2020-09-10T16:20:10Z",
      "message": "cluster is reachable",
      "reason": "ClusterReachable",
      "status": "True",
      "type": "ActiveAPIURLOverride"
    }
    ```

## Attach the shard to a region

Once all the validations have been completed the shard is ready to be associated with providers and regions. Take a look into other shards configs to see the syntax of the `cloud_providers` section

### Verify that at least one round of osde2e tests ran successfully when using the new shard. Dashboards:

osd2e2e tests use `osdctl` to create new clusters periodically and run validations on top of them. At the moment `osdctl` does not support to specify a shard, so these tests will be run once we have attached the shard to a region. Once that is done, wait a few hours and check the following urls to see a cluster associated in the new shard passing osde2e tests.

- https://prow.ci.openshift.org/job-history/gs/origin-ci-test/logs/osde2e-prod-aws-e2e-default
- https://openshift.github.io/osde2e/

## OSD operators notes

#### aws-account-operator

- (SREP) AWS account pool creation: this takes some time because AWS has to enable enterprise support before accounts can be used

1. Confirm the total number of used accounts

    Make sure there are at least 300 accounts left in the payer account. To check it, run the command above from the payer account and compare it against the account limit per payer account. The account limit is 5000 in production, and the shared stage/integration limit has been increased to 7500 by AWS.

    ```
    $ aws organizations list-accounts | jq '.Accounts[].Name' | wc -l
    ```

1. Make sure the operator is up and running in the new shard

    Check operator logs and confirm there are no errors

    ```
    $ oc get pods -n aws-account-operator
    ```

1. Deploy an AccountPool CR in the new hive shard

    Apply to the new shard an [AccountPool CR](https://github.com/openshift/aws-account-operator/blob/master/deploy/crds/aws_v1alpha1_accountpool_cr.yaml) with PoolSize = 50

    [Card to automate this step](https://issues.redhat.com/browse/OSD-4602)

1. Apply AWSFederatedRoles

    Apply all AWSFederatedRoles from the [deploy/crds](https://github.com/openshift/aws-account-operator/tree/master/deploy/crds) folder, using:

    ```
    $ osdctl federatedrole apply -f aws_v1alpha1_awsfederatedrole_networkmgmt_cr.yaml
    $ osdctl federatedrole apply -f aws_v1alpha1_awsfederatedrole_readonly_cr.yaml
    ```

    [Card to automate this step](https://issues.redhat.com/browse/OSD-4603)

1. Validate - Confirm accounts have been created to fill the Pool

    Check the account CRs in the new shard have been created, initialized and have EnterpriseSupport on, by checking the account CRs. Account creation and initialization takes around 1min/account, so initializing the initial pool of 50 accounts will take close to 1hour. After this period you’ll notice the account CRs will be moved to the PendingVerification status:

    ```
    $ oc get accounts -n aws-account-operator
    osd-creds-mgmt-wqdf9f   PendingVerification                                 13m
    osd-creds-mgmt-wsszvm   PendingVerification                                 14m
    osd-creds-mgmt-wzwpd9   PendingVerification                                14m
    osd-creds-mgmt-x78rz9   PendingVerification                                13m
    osd-creds-mgmt-x7jh4m   PendingVerification                                13m
    osd-creds-mgmt-xvt6h9   PendingVerification                                13m
    osd-creds-mgmt-zmrdht   PendingVerification                                 11m
    osd-creds-mgmt-zrfxh6   PendingVerification                                 11m
    osd-creds-mgmt-zz8tzf   PendingVerification                                 13m
    osd-creds-mgmt-zzfz9j   PendingVerification                                 10m
    ```

    In the **PendingVerification** status, the account is pending on action from AWS, processing a SupportCase the operator creates automatically to request EnterpriseSupport to be turned on. It's a manual operation performed in Amazon, so the time it takes can vary (from 30 minutes to 4 hours).

    When adding a production shard we should wait for all the accounts to be initialized and processed by AWS before turning on scheduling clusters to it.

#### certman-operator

We are re-using the same ConfigMap across all shards

We are re-using the same Secrets across all shards

#### deadmanssnitch-operator

We are re-using the same secrets (same API key) across all shards

#### gcp-project-operator

We are re-using the same secrets (gcp^credentials) across all shards

#### pagerduty-operator

It is known and acknowledged that we are starting with using the same key for the different shards, so there is no extra steps needed for pagerduty-operator once namespaces/CR are deployed through app-interface.

#### splunk-forwarder-operator

We are re-using the same secrets across all shards
