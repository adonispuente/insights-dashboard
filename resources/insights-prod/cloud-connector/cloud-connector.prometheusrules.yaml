---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cloud-connector
  labels:
    prometheus: app-sre
    role: alert-rules
spec:
  groups:
  - name: cloud-connector
    rules:

    - alert: CloudConnectorAbsent
      expr: |
        absent(up{job="cloud-connector-api"} == 1) OR
        absent(up{job="cloud-connector-mqtt-message-consumer"} == 1) OR
        absent(up{job="cloud-connector-kafka-message-consumer"} == 1)
      for: 5m
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments/cloud-connector-api"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/console.redhat.com/app-sops/cloud-connector/App-insights-cloud-connector-api-In-cloud-connector-prod-Absent.rst"
        message: "Cloud-Connector Alert. Cloud-Connector pod has been down for 5 minutes."


    - alert: CloudConnectorRestart
      expr: |
        sum(increase(kube_pod_container_status_restarts_total{container=~"cloud-connector-.*"}[5m])) by (container) > 0
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments/cloud-connector-api"
        message: "Cloud-Connector Alert. Cloud-Connector container {{ $labels.container }} has been restarting."


    - alert: CloudConnectorAPIErrors
      expr: |
          1 - (sum(increase(cloud_connector_http_status_code_counter{status_code=~"5.*", service="cloud-connector-api"}[1w])) / sum(increase(cloud_connector_http_status_code_counter{service="cloud-connector-api"}[1w]))) < 0.95
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments/cloud-connector-api"
        message: "Cloud-Connector API Alert.  Cloud-Connector API has exceeded 5% 5XX API response quota."


    - alert: CloudConnectorMQTTPublishError
      expr: |
        sum(increase(cloud_connector_mqtt_message_published_failure_count[5m])) > 0
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://kibana.apps.crcp01ue1.o9m8.p1.openshiftapps.com/goto/65ca3ce87c38e6cb2ef5e806fdb8252c"
        message: "Cloud-Connector Alert.  MQTT message publish failure rate is high."


    - alert: CloudConnectorInventoryKafkaProducerErrors
      expr: |
        sum(increase(cloud_connector_inventory_kafka_writer_failure_count[5m])) > 0
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://kibana.apps.crcp01ue1.o9m8.p1.openshiftapps.com/goto/65ca3ce87c38e6cb2ef5e806fdb8252c"
        message: "Cloud-Connector Alert.  Inventory message publish failure rate is high."


    - alert: CloudConnectorInventoryStaleTimestampUpdaterFailure
      expr: sum(sum_over_time(kube_pod_status_phase{namespace=~"cloud-connector-.*", phase="Failed", pod=~"cloud-connector-stale-timestamp-updater-.*"}[30m:5m]) < 3) by (pod, namespace) > 0
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments"
        message: '{{ $labels.namespace }} Inventory Stale Timestamp Updater Failed | {{ $labels.pod }} failed to run'


    - alert: CloudConnectorConnectionCountReporterFailure
      expr: sum(sum_over_time(kube_pod_status_phase{phase="Failed", namespace=~"cloud-connector-.*", pod=~"cloud-connector-connection-per-account-reporter-.*"}[30m:5m]) < 3) by (pod, namespace) > 0
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments"
        message: '{{ $labels.namespace }} Connection Count Reporter Failed | {{ $labels.pod }} failed to run'


    - alert: CloudConnectorIngressKafkaConsumerLag
      expr: sum(avg_over_time(kafka_consumergroup_group_lag{group="cloud-connector-rhc-message-consumer", topic="platform.cloud-connector.rhc-message-ingress"} [10m])) by (topic) > 1000
      for: 95m
      labels:
        severity: info
        service: insights
        env: prod
        app_team: data-pipeline
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/c91dcda039618ded/cloud-connector?orgId=1&var-datasource=crcp01ue1-prometheus"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/cloud-connector-prod/deployments"
        message: '{{ $labels.topic }} Cloud Connector Ingress Kafka Consumer Lag is high'
