---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: insights-advisor-prod
  labels:
    prometheus: app-sre
    role: alert-rules
spec:
  groups:
  - name: insights-advisor-prod
    rules:
      - alert: App-insights-advisor-api-In-advisor-prod-Downtime-Quota-Reached
        expr: avg(avg_over_time(up{service="insights-advisor-api", namespace="advisor-prod"}[24h])) <= 0.95
        for: 5m
        labels:
          severity: medium
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://grafana.app-sre.devshift.net/d/wUa2bXdMz/advisor-api"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments/insights-advisor-api"
          message: "Insights Advisor API Alert. Insights Advisor API has exceeded its 2% downtime quota for the past 24 hours."
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-api-In-advisor-prod-Downtime-Quota-Reached.rst"

      - alert: App-insights-advisor-service-In-advisor-prod-Downtime-Quota-Reached
        expr: avg(avg_over_time(up{service="insights-advisor-service", namespace="advisor-prod"}[24h])) <= 0.98
        for: 5m
        labels:
          severity: critical
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://grafana.app-sre.devshift.net/d/s9df5udMk/advisor-service"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments/insights-advisor-service"
          message: "Insights Advisor Service Alert. Insights Advisor Service has exceeded its 2% downtime quota for the past 24 hours."
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-service-In-advisor-prod-Downtime-Quota-Reached.rst"

      - alert: App-insights-advisor-api-In-advisor-prod-5XX-Quota-Reached
        expr: sum(django_http_responses_total_by_status_total{service="insights-advisor-api", status!~"5.*"}) / sum(django_http_responses_total_by_status_total{service="insights-advisor-api"}) <= 0.95
        for: 5m
        labels:
          severity: critical
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://grafana.app-sre.devshift.net/d/wUa2bXdMz/advisor-api"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments/insights-advisor-api"
          message: "Insights Advisor API Alert. Insights Advisor API has exceeded 5% 5XX response quota."
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-api-In-advisor-prod-5XX-Quota-Reached.rst"

      - alert: App-insights-advisor-service-In-advisor-prod-Failed-Processing-Quota-Reached
        expr: sum(increase(insights_advisor_service_successful_requests_total[24h])) / sum(increase(insights_advisor_service_total_requests_total[24h])) <= 0.95
        for: 5m
        labels:
          severity: critical
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://grafana.app-sre.devshift.net/d/s9df5udMk/advisor-service"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments/insights-advisor-service"
          message: "Insights Advisor Service Alert. Insights Advisor Service has exceeded 5% failure quota for processing engine results from Kafka in the last 24 hours."
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-service-In-advisor-prod-Failed-Processing-Quota-Reached.rst"

      - alert: App-insights-advisor-api-In-advisor-prod-Absent
        expr: absent(up{service="insights-advisor-api", namespace="advisor-prod"}) or up{service="insights-advisor-api", namespace="advisor-prod"} == 0
        for: 5m
        labels:
          severity: critical
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://GRAFANA_URL"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments"
          message: "Metrics for insights-advisor-api in PROD are absent"
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-api-in-advisor-prod-Absent.rst"

      - alert: App-insights-advisor-service-In-advisor-prod-Absent
        expr: absent(up{service="insights-advisor-service", namespace="advisor-prod"}) or up{service="insights-advisor-service", namespace="advisor-prod"} == 0
        for: 5m
        labels:
          severity: critical
          service: insights
          env: prod
          app_team: advisor
        annotations:
          dashboard: "https://GRAFANA_URL"
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/deployments"
          message: "Metrics for insights-advisor-service in PROD are absent"
          description: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/cloud.redhat.com/app-sops/advisor/App-insights-advisor-service-in-advisor-prod-Absent.rst"

      - alert: AdvisorWeeklyEmailsFailed
        expr: advisor_weekly_report_emails_status == 0
        labels:
          severity: medium
          service: insights
          env: prod
          app_team: advisor
        annotations:
          link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/advisor-prod/pods?name=weekly-report-emails"
          message: "Sending weekly report emails failed: {{ $labels.status_message }}.  Refer to the pod logs for more details"
