---
$schema: /app-interface/prometheus-rule-test-1.yml

rule_files:
- /observability/prometheusrules/uhc-clusters-service-stage.prometheusrules.yaml

evaluation_interval: 1m

tests:

# Check that the rate limits alert is triggered when the limited calls metric
# increases:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="stage:clusters_mgmt"}
    values: 0 0 0 1 1

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - stage
    exp_alerts:
    - exp_labels:
        email: service-development-a
        service: uhc-clusters-service
        severity: info
      exp_annotations:
        message: OCM stage clusters management service is rejecting requests due to exceeded rate limits
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the rate limits alert isn't triggered when the limited calls
# metric doesn't increase:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="stage:clusters_mgmt"}
    values: 1 1 1 1 1

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - stage
    exp_alerts: []

# Check that the rate limits alert isn't triggered when the limited calls
# metric is zero:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="stage:clusters_mgmt"}
    values: 0 0 0 0 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - stage
    exp_alerts: []

# Check that the rate limits alert isn't triggered when the limited calls
# metric is reset:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="stage:clusters_mgmt"}
    values: 1 1 1 0 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - stage
    exp_alerts: []

- interval: 1m
  ## 5 minutes without 5xx and normal 2xx and then we start seeing 5xx for the next 15 minutes 
  input_series:
    # series come from the two routers
  - series: haproxy_backend_http_responses_total{backend="https",code="5xx",container="router",endpoint="metrics",exported_namespace="uhc-stage",instance="10.128.6.17:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-bhswd",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 100+0x5 100+7x15
  - series: haproxy_backend_http_responses_total{backend="https",code="5xx",container="router",endpoint="metrics",exported_namespace="uhc-stage",instance="10.129.6.10:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-6qz58",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 100+0x5 100+7x15
  - series: haproxy_backend_http_responses_total{backend="https",code="2xx",container="router",endpoint="metrics",exported_namespace="uhc-stage",instance="10.128.6.17:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-bhswd",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 1000+100x20
  - series: haproxy_backend_http_responses_total{backend="https",code="2xx",container="router",endpoint="metrics",exported_namespace="uhc-stage",instance="10.129.6.10:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-6qz58",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 1000+100x20

  alert_rule_test:
  - eval_time: 5m
    alertname: UHC clusters service 5xx Errors High - stage
    exp_alerts: []
  - eval_time: 20m
    alertname: UHC clusters service 5xx Errors High - stage
    exp_alerts:
    - exp_labels:
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: "UHC clusters service is returning errors for 6.542% of requests."
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop"

# Check that the crashing alert fires when there are two restarts in the same
# container and pod in less than ten minutes:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-stage",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - stage
    exp_alerts:
    - exp_labels:
        pod: clusters-service-123-456
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM stage clusters management service container 'service' of pod 'clusters-service-123-456' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the crashing alert doesn't fire when there is only one restart in
# the last ten minutes:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-stage",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - stage
    exp_alerts: []

# Check that the crashing alert fires independently for different pods:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-stage",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2
  - series: kube_pod_container_status_restarts_total{namespace="uhc-stage",pod="clusters-service-123-789",container="service"}
    values: 0 0 0 1 2

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - stage
    exp_alerts:
    - exp_labels:
        pod: clusters-service-123-456
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM stage clusters management service container 'service' of pod 'clusters-service-123-456' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop
    - exp_labels:
        pod: clusters-service-123-789
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM stage clusters management service container 'service' of pod 'clusters-service-123-789' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the crashing alert clears after ten minutes without restarts:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-stage",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2

  alert_rule_test:
  - eval_time: 15m
    alertname: OCMClustersMgmtCrashing - stage
    exp_alerts: []
