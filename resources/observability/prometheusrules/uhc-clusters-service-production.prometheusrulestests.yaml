---
$schema: /app-interface/prometheus-rule-test-1.yml

rule_files:
- /observability/prometheusrules/uhc-clusters-service-production.prometheusrules.yaml

evaluation_interval: 1m

tests:

# Check that the rate limits alert is triggered when the limited calls metric
# increases:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="production:clusters_mgmt"}
    values: 0 0 0 1 1

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - production
    exp_alerts:
    - exp_labels:
        email: service-development-a
        service: uhc-clusters-service
        severity: info
      exp_annotations:
        message: OCM production clusters management service is rejecting requests due to exceeded rate limits
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the rate limits alert isn't triggered when the limited calls
# metric doesn't increase:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="production:clusters_mgmt"}
    values: 1 1 1 1 1

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - production
    exp_alerts: []

# Check that the rate limits alert isn't triggered when the limited calls
# metric is zero:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="production:clusters_mgmt"}
    values: 0 0 0 0 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - production
    exp_alerts: []

# Check that the rate limits alert isn't triggered when the limited calls
# metric is reset:
- interval: 1m
  input_series:
  - series: limited_calls{namespace="app-sre-rate-limiting",service="limitador",limitador_namespace="production:clusters_mgmt"}
    values: 1 1 1 0 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtRateLimits - production
    exp_alerts: []

- interval: 1m
  ## 5 minutes without 5xx and normal 2xx and then we start seeing 5xx for the next 15 minutes
  input_series:
    # series come from the two routers
  - series: haproxy_backend_http_responses_total{backend="https",code="5xx",container="router",endpoint="metrics",exported_namespace="uhc-production",instance="10.128.6.17:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-bhswd",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 100+0x5 100+7x15
  - series: haproxy_backend_http_responses_total{backend="https",code="5xx",container="router",endpoint="metrics",exported_namespace="uhc-production",instance="10.129.6.10:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-6qz58",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 100+0x5 100+7x15
  - series: haproxy_backend_http_responses_total{backend="https",code="2xx",container="router",endpoint="metrics",exported_namespace="uhc-production",instance="10.128.6.17:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-bhswd",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 1000+100x20
  - series: haproxy_backend_http_responses_total{backend="https",code="2xx",container="router",endpoint="metrics",exported_namespace="uhc-production",instance="10.129.6.10:1936",job="router-internal-default",namespace="openshift-ingress",pod="router-default-69c7459f76-6qz58",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-1",route="clusters-mgmt",service="router-internal-default"}
    values: 1000+100x20

  alert_rule_test:
  - eval_time: 5m
    alertname: UHC clusters service 5xx Errors High - production
    exp_alerts: []
  - eval_time: 20m
    alertname: UHC clusters service 5xx Errors High - production
    exp_alerts:
    - exp_labels:
        email: service-development-a
        service: uhc-clusters-service
        severity: critical
      exp_annotations:
        message: "UHC clusters service is returning errors for 6.542% of requests."
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop"

# Check that the crashing alert fires when there are two restarts in the same
# container and pod in less than ten minutes:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-production",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - production
    exp_alerts:
    - exp_labels:
        pod: clusters-service-123-456
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM production clusters management service container 'service' of pod 'clusters-service-123-456' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the crashing alert doesn't fire when there is only one restart in
# the last ten minutes:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-production",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 0

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - production
    exp_alerts: []

# Check that the crashing alert fires independently for different pods:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-production",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2
  - series: kube_pod_container_status_restarts_total{namespace="uhc-production",pod="clusters-service-123-789",container="service"}
    values: 0 0 0 1 2

  alert_rule_test:
  - eval_time: 5m
    alertname: OCMClustersMgmtCrashing - production
    exp_alerts:
    - exp_labels:
        pod: clusters-service-123-456
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM production clusters management service container 'service' of pod 'clusters-service-123-456' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop
    - exp_labels:
        pod: clusters-service-123-789
        container: service
        email: service-development-a
        service: uhc-clusters-service
        severity: medium
      exp_annotations:
        message: OCM production clusters management service container 'service' of pod 'clusters-service-123-789' is crashing
        runbook: https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/uhc/sop

# Check that the crashing alert clears after ten minutes without restarts:
- interval: 1m
  input_series:
  - series: kube_pod_container_status_restarts_total{namespace="uhc-production",pod="clusters-service-123-456",container="service"}
    values: 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2

  alert_rule_test:
  - eval_time: 15m
    alertname: OCMClustersMgmtCrashing - production
    exp_alerts: []

- interval: 1m
  input_series:
  - series: aws_rds_free_storage_space_average{container="cloudwatch-exporter", dbinstance_identifier="clusters-service-production", namespace="uhc-production"}
    values: '2147483648+0x59 536870912+0x119' # 2Gb free for 1 hour then only 0.5 Gb free for next 2 hours

  alert_rule_test:
  - eval_time: 1h
    alertname: ClustersServiceRDSLowStorageSpace
    exp_alerts:
  - eval_time: 3h
    alertname: ClustersServiceRDSLowStorageSpace
    exp_alerts:
    - exp_labels:
        container: cloudwatch-exporter
        dbinstance_identifier: clusters-service-production
        namespace: uhc-production
        service: uhc-clusters-service
        severity: critical
      exp_annotations:
        message: 'The current free storage space of DB instance clusters-service-production is under 1 GB.'
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/blob/master/docs/uhc/sop/ocm-cluster-service.md#ClustersServiceRDSLowStorageSpace"
        dashboard: "https://grafana.app-sre.devshift.net/d/AWSRDSdbi/aws-rds?orgId=1&var-datasource=AWS%20app-sre&var-region=default&var-dbinstanceidentifier=clusters-service-production&from=now-7d&to=now"

- interval: 1m
  input_series:
  - series: aws_rds_free_storage_space_average{container="cloudwatch-exporter", dbinstance_identifier="clusters-service-production", namespace="uhc-production"}
    values: '2147483648+0x59 536870912+0x119' # 2Gb free for 1 hour then only 0.5 Gb free for next 2 hours

  alert_rule_test:
  - eval_time: 1h
    alertname: ClustersServiceRDSLowStorageSpace
    exp_alerts:
  - eval_time: 3h
    alertname: ClustersServiceRDSLowStorageSpace
    exp_alerts:
    - exp_labels:
        container: cloudwatch-exporter
        dbinstance_identifier: clusters-service-production
        namespace: uhc-production
        service: uhc-clusters-service
        severity: critical
      exp_annotations:
        message: 'The current free storage space of DB instance clusters-service-production is under 1 GB.'
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/blob/master/docs/uhc/sop/ocm-cluster-service.md#ClustersServiceRDSLowStorageSpace"
        dashboard: "https://grafana.app-sre.devshift.net/d/AWSRDSdbi/aws-rds?orgId=1&var-datasource=AWS%20app-sre&var-region=default&var-dbinstanceidentifier=clusters-service-production&from=now-7d&to=now"

# RDSOutOfStorageSoon
- interval: 10m
  input_series:
  - series: rdsosmetrics_fileSys_usedPercent{mount_point="/rdsdbdata", exported_instance="clusters-service-production"}
    values: 0+3x4 12-1x7
  alert_rule_test:
  - eval_time: 2h
    alertname: ClustersServiceRDSOutOfStorageSoon

- interval: 10m
  input_series:
  - series: rdsosmetrics_fileSys_usedPercent{mount_point="/rdsdbdata", exported_instance="clusters-service-production"}
    values: 0+3x12
  alert_rule_test:
  - eval_time: 2h
    alertname: ClustersServiceRDSOutOfStorageSoon
    exp_alerts:
    - exp_labels:
        severity: critical
        service: uhc-clusters-service
        env: production
        exported_instance: clusters-service-production
        mount_point: /rdsdbdata
      exp_annotations:
          message: "DB instance clusters-service-production is about to run out of storage within 2 weeks."
          runbook: "https://gitlab.cee.redhat.com/service/app-interface/blob/master/docs/uhc/sop/ocm-cluster-service.md#ClustersServiceRDSOutOfStorageSoon"
          link_url: "https://console.aws.amazon.com/rds/home?region=us-east-1#database:id=clusters-service-production;is-cluster=false;tab=monitoring"
          dashboard: "https://grafana.app-sre.devshift.net/d/statusboard/status-board?orgId=1&var-datasource=app-sre-prod-04-prometheus"
