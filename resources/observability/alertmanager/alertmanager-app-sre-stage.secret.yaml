apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-app-sre
data:
  alertmanager.yaml: >-
    {{% b64encode %}}
    # Alertmanager routing configuration
    global:
      resolve_timeout: 5m
      # Global Pagerduty URL
      pagerduty_url: https://pagerduty.example.com

      # Email alerts default configuration:
      smtp_from: {{{ vault('app-sre/creds/smtp', 'username') }}}
      smtp_smarthost: {{{ vault('app-sre/creds/smtp', 'server') }}}:{{{ vault('app-sre/creds/smtp', 'port') }}}
      smtp_auth_username: {{{ vault('app-sre/creds/smtp', 'username') }}}
      smtp_auth_password: {{{ vault('app-sre/creds/smtp', 'password') }}}
      smtp_auth_identity: {{{ vault('app-sre/creds/smtp', 'username') }}}

      # Global slack configuration:
      slack_api_url: {{{ vault('app-interface/app-sre/app-sre-observability-production/alertmanager-integration', 'slack_api_url') }}}


    # The root route on which each incoming alert enters.
    route:
      # The labels by which incoming alerts are grouped together. For example,
      # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
      # be batched into a single group.
      #
      # To aggregate by all possible labels use '...' as the sole label name.
      # This effectively disables aggregation entirely, passing through all
      # alerts as-is. This is unlikely to be what you want, unless you have
      # a very low alert volume or your upstream notification system performs
      # its own grouping. Example: group_by: [...]
      group_by: ['job', 'cluster', 'service']
      # When a new group of alerts is created by an incoming alert, wait at
      # least 'group_wait' to send the initial notification.
      # This way ensures that you get multiple alerts for the same group that start
      # firing shortly after another are batched together on the first
      # notification.
      group_wait: 30s
      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.
      group_interval: 5m
      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.
      repeat_interval: 24h
      # The default receiver; For staging, all alerts go to a single channel
      receiver: slack-app-sre-alerts-staging
      routes:
      # Route them to correct snitch otherwise
      - match:
          severity: "deadman"
        group_wait: 30s
        group_interval: 1m
        repeat_interval: 1m
        receiver: slack-app-sre-alerts-staging
        # Also group alerts by affected service
        group_by: [cluster,prometheus]
        routes:
        - match:
            cluster: app-sre
          receiver: deadmanssnitch-app-sre
        - match:
            cluster: blr
          receiver: deadmanssnitch-bangalore
        - match:
            cluster: centralci
          receiver: deadmanssnitch-centralci

      # Route testing alerts
      - match:
          severity: test
        receiver: slack-app-sre-alerts-test
        # Also group alerts by affected service
        group_by: [alertname, cluster, job, service]
        
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'medium'
      # Apply inhibition if the alertname is the same.
      equal: ['alertname', 'cluster', 'service']

    templates:
    - '*.tmpl'
    - '/etc/alertmanager/configmaps/templates/*.tmpl'

    receivers:
    # Slack receivers
    # Default Receiver: Send all alerts with non-specific route to sd-app-sre-alert-stg alerting channel
    - name: slack-app-sre-alerts-staging
      slack_configs:
      # app-sre-alerts-stg channel. Owner: sd-app-sre@redhat.com
      - channel: '#sd-app-sre-alert-stg'
        send_resolved: true
        username: 'app-sre-alerts'
        title: '{{ template "slack.default.title" . }}'
        icon_emoji: '{{ template "slack.default.icon_emoji" . }}'
        color: '{{ template "slack.default.color" . }}'
        text: '{{ template "slack.default.text" . }}'
        actions:
        - type: button
          text: 'Runbook :green_book:'
          url: '{{ (index .Alerts 0).Annotations.runbook }}'
        - type: button
          text: 'Query :mag:'
          url: '{{ (index .Alerts 0).GeneratorURL }}'
        - type: button
          text: 'Dashboard :grafana:'
          url: '{{ (index .Alerts 0).Annotations.dashboard }}'
        - type: button
          text: 'Silence :no_bell:'
          url: '{{ template "__alert_silence_link" . }}'
        - type: button
          text: '{{ template "slack.default.link_button_text" . }}'
          url: '{{ .CommonAnnotations.link_url }}'

    - name: slack-app-sre-alerts-test
      slack_configs:
      - channel: '#sd-app-sre-alert-test'
        send_resolved: true
        username: 'app-sre-alerts'
        title: '{{ template "slack.default.title" . }}'
        icon_emoji: '{{ template "slack.default.icon_emoji" . }}'
        color: '{{ template "slack.default.color" . }}'
        text: '{{ template "slack.default.text" . }}'
        actions:
        - type: button
          text: 'Runbook :green_book:'
          url: '{{ (index .Alerts 0).Annotations.runbook }}'
        - type: button
          text: 'Query :mag:'
          url: '{{ (index .Alerts 0).GeneratorURL }}'
        - type: button
          text: 'Dashboard :grafana:'
          url: '{{ (index .Alerts 0).Annotations.dashboard }}'
        - type: button
          text: 'Silence :no_bell:'
          url: '{{ template "__alert_silence_link" . }}'
        - type: button
          text: '{{ template "slack.default.link_button_text" . }}'
          url: '{{ .CommonAnnotations.link_url }}'

    # Deadmanssnitch receivers
    - name: deadmanssnitch-app-sre
      webhook_configs:
      - url: {{{ vault('app-interface/app-sre/app-sre-observability-production/alertmanager-integration', 'deadmanssnitch-app-sre-url')}}}

    - name: deadmanssnitch-bangalore
      webhook_configs:
      - url: {{{ vault('app-interface/app-sre/app-sre-observability-production/alertmanager-integration', 'deadmanssnitch-bangalore-url')}}}

    - name: deadmanssnitch-centralci
      webhook_configs:
      - url: {{{ vault('app-interface/app-sre/app-sre-observability-production/alertmanager-integration', 'deadmanssnitch-centralci-url')}}}
    {{% endb64encode %}}
  chat.tmpl: >-
    {{% b64encode %}}
    # This builds the silence URL.  We exclude the alertname in the range
    # to avoid the issue of having trailing comma separator (%2C) at the end
    # of the generated URL
    {{ define "__alert_silence_link" -}}
        {{ .ExternalURL }}/#/silences/new?filter=%7B
        {{- range .CommonLabels.SortedPairs -}}
            {{- if ne .Name "alertname" -}}
                {{- .Name }}%3D"{{- .Value -}}"%2C%20
            {{- end -}}
        {{- end -}}
        alertname%3D"{{ .CommonLabels.alertname }}"%7D
    {{- end }}


    {{ define "__alert_severity_prefix" -}}
        {{ if ne .Status "firing" -}}
        :green_apple:
        {{- else if eq .Labels.severity "critical" -}}
        :fire:
        {{- else if eq .Labels.severity "warning" -}}
        :warning:
        {{- else if eq .Labels.severity "medium" -}}
        :warning:
        {{- else if eq .Labels.severity "test" -}}
        :grey_exclamation:
        {{- else -}}
        :question:
        {{- end }}
    {{- end }}

    {{ define "__alert_severity_prefix_title" -}}
        {{ if ne .Status "firing" -}}
        :green_apple:
        {{- else if eq .Labels.severity "critical" -}}
        :fire:
        {{- else if eq .Labels.severity "warning" -}}
        :warning:
        {{- else if eq .Labels.severity "medium" -}}
        :warning:
        {{- else if eq .Labels.severity "test" -}}
        :grey_exclamation:
        {{- else -}}
        :question:
        {{- end }}
    {{- end }}

    {{ define "__single_message_title" }}{{ range .Alerts.Firing }}{{- if .Annotations.message }} {{ .Annotations.message }} {{- end }}{{ end }}{{ range .Alerts.Resolved }}{{- if .Annotations.message }} {{ .Annotations.message }} {{- end }}
    {{ end }}{{ end }}

    {{/* First line of Slack alerts */}}
    {{ define "slack.default.title" -}}Alert: {{ .CommonLabels.alertname }} [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}{{ template "__single_message_title" . }}{{ end }}{{- end }}


    {{/* Color of Slack attachment (appears as line next to alert )*/}}
    {{ define "slack.default.color" -}}
        {{ if eq .Status "firing" -}}
            {{ if eq .CommonLabels.severity "test" -}}
                '#808080'
            {{ else if eq .CommonLabels.severity "warning" -}}
                warning
            {{ else if eq .CommonLabels.severity "medium" -}}
                warning
            {{- else if eq .CommonLabels.severity "critical" -}}
                danger
            {{- else -}}
                '#439FE0'
            {{- end -}}
        {{ else -}}
        good
        {{- end }}
    {{- end }}


    {{/* Emoji to display as user icon (custom emoji supported!) */}}
    {{ define "slack.default.icon_emoji" }}:prometheus:{{ end }}

    {{ define "slack.default.icon_url" }}https://avatars3.githubusercontent.com/u/3380462{{ end }}

    {{/* The text to display in the alert */}}
    {{/* define "slack.default.text" -}}{{ range .Alerts }}{{ if eq .Status "firing" -}}{{- if .Annotations.message }}{{ .Annotations.message }}{{- end }}{{- if .Annotations.description }}{{ .Annotations.description }}{{- end }}{{- end }}{{ if eq .Status "resolved" -}}{{- if .Annotations.message }}Resolved: {{ .Annotations.message }}{{- end }}{{- end }}{{- end }}{{- end */}}

    {{ define "slack.default.text" }}
    {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
    {{ range .Alerts.Firing }}{{ .Annotations.link_url }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.link_url }}{{ end }}
    {{ range .Alerts.Firing }}{{ .Annotations.description }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.description }}{{ end }}
    {{ else }}
    {{ if gt (len .Alerts.Firing) 0 }}
    *Alerts Firing:*
    {{ range .Alerts.Firing }}- {{- if .Annotations.message }} {{ .Annotations.message }} {{- end }}{{- if .Annotations.link_url }}: {{ .Annotations.link_url }}{{- end }}
    {{ end }}{{ end }}
    {{ if gt (len .Alerts.Resolved) 0 }}
    *Alerts Resolved:*
    {{ range .Alerts.Resolved }}- {{- if .Annotations.message }} {{ .Annotations.message }} {{- end }}{{- if .Annotations.link_url }}: {{ .Annotations.link_url }}{{- end }}
    {{ end }}{{ end }}
    {{ end }}
    {{ end }}

    {{- /* If none of the below matches, send to #sd-app-sre-alert-test, and we 
    can then assign the expected code_owner to the alert or map the code_owner
    to the correct channel */ -}}
    {{ define "__get_channel_for_code_owner" -}}
        {{- if eq . "app-sre" -}}
            sd-app-sre-alert
        {{- else if eq . "telemeter" -}}
            forum-monitoring
        {{- else -}}
            sd-app-sre-alert-test
        {{- end -}}
    {{- end }}

    {{- /* Select the channel based on the code_owner. We only expect to get
    into this template function if the code_owners label is present on an alert.
    This is to defend against us accidentally breaking the routing logic. */ -}}
    {{ define "slack.default.code_owner_channel" -}}
        {{- if .CommonLabels.code_owner }}
            {{ template "__get_channel_for_code_owner" .CommonLabels.code_owner }}
        {{- else -}}
            monitoring
        {{- end }}
    {{- end }}

    {{ define "slack.default.link_button_text" -}}
        {{- if .CommonAnnotations.link_text -}}
            {{- .CommonAnnotations.link_text -}}
        {{- else -}}
            Link
        {{- end }} :link:
    {{- end }}
    {{% endb64encode %}}
