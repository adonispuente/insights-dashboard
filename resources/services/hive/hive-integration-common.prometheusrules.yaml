---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: hive-integration-{{{shard_name}}}
spec:
  groups:
  - name: hive-integration-{{{shard_name}}}
    rules:
    - alert: HiveControllersDown - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-controllers in namespace {{ $labels.namespace }}. hive-controllers is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="hive-controllers"} == 1)
      for: 5m
      labels:
        service: hive
        severity: high
    - alert: HiveClusterSyncDown - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-clustersync in namespace {{ $labels.namespace }}. hive-clustersync is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="hive-clustersync"} == 1)
      for: 5m
      labels:
        service: hive
        severity: high
    - alert: HiveOperatorDown - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-operator in namespace {{ $labels.namespace }}. hive-operator is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="openshift-customer-monitoring/hive-operator"} == 1)
      for: 5m
      labels:
        service: hive
        severity: high
    - alert: HiveDeploymentFailed - integration - {{{shard_name}}}
      annotations:
        message: "hive deployment has failed. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(hive_hiveconfig_conditions{condition="Ready"} == 1)
      for: 5m
      labels:
        service: hive
        severity: high  
    - alert: ClusterServiceVersionStuck - integration - {{{shard_name}}}
      annotations:
        message: "ClusterServiceVersion {{ $labels.name }} stuck in abnormal state: {{ $labels.reason }}"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/hive/sop/hive-olm-dance.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        csv_abnormal{exported_namespace="hive"}
      for: 10m
      labels:
        service: hive
        severity: medium
    - alert: InstallJobDelayHigh - integration - {{{shard_name}}}
      annotations:
        message: "Time to start an install job is taking greater than 5 minutes"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/InstallJobDelayHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_cluster_deployment_install_job_delay_seconds_sum{job="hive-controllers"}[6h])
        /
        rate(hive_cluster_deployment_install_job_delay_seconds_count{job="hive-controllers"}[6h]) > 300
      for: 10m
      labels:
        service: hive
        severity: medium
    - alert: ControllerErrorsHigh - integration - {{{shard_name}}}
      annotations:
        message: "{{ $labels.controller }} controller is reporting a high error rate: {{ $value }}/s"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(controller_runtime_reconcile_errors_total{job="hive-controllers"}[15m]) > 1
      for: 10m
      labels:
        service: hive
        severity: high
    - alert: LocalKubeClientRequestsHigh - POST requests - integration - {{{shard_name}}}
      annotations:
        message: "detected {{ $value }} {{ $labels.resource }} local POST API requests per second from controller {{ $labels.controller }}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/KubeClientRequestsHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_kube_client_requests_total{job="hive-controllers",remote="false",method="POST"}[5m]) > 15
      for: 1m
      labels:
        service: hive
        severity: high
    - alert: LocalKubeClientRequestsHigh - non-POST requests - integration - {{{shard_name}}}
      annotations:
        message: "detected {{ $value }} {{ $labels.resource }} local non-POST API requests per second from controller {{ $labels.controller }}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/KubeClientRequestsHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_kube_client_requests_total{job="hive-controllers",remote="false",method!~"POST"}[5m]) > 15
      for: 1m
      labels:
        service: hive
        severity: medium
    - alert: ClusterProvisioningDelaySREP - integration - {{{shard_name}}}
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.exported_namespace }} provisioning taking over 2 hours. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ClusterProvisioningDelay.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        (hive_cluster_deployment_provision_underway_seconds{job="hive-controllers",cluster_type="managed"}/3600) > 2
      for: 5m
      labels:
        service: hive
        severity: medium
        team: srep
    - alert: ClusterProvisioningDelayHive - integration - {{{shard_name}}}
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.exported_namespace }} provisioning taking over 2 hours. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}. "
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ClusterProvisioningDelay.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      # TODO: for hive team we skip alerts if it's been going over 24h, and if a reason has been auto-detected
      expr: |
        (hive_cluster_deployment_provision_underway_seconds{job="hive-controllers",reason="Unknown"}/3600) > 2
        and
        (hive_cluster_deployment_provision_underway_seconds{job="hive-controllers",reason="Unknown"}/3600) < 24
      for: 5m
      labels:
        service: hive
        severity: medium
        team: hive
    - alert: ClusterProvisioningFailure - integration - {{{shard_name}}}
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.exported_namespace }} provisioning failed after 3 times retries. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        (hive_cluster_deployment_provision_underway_install_restarts{job="hive-controllers",cluster_type="managed"}) > 2
      for: 1m
      labels:
        service: hive
        severity: medium
    - alert: ClusterDeprovisioningDelay - integration - {{{shard_name}}}
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.namespace }} deprovisioning taking over 6 hours"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ClusterDeprovisioningDelay.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        (hive_cluster_deployment_deprovision_underway_seconds{job="hive-controllers"} / 3600 ) > 6
      for: 10m
      labels:
        severity: medium
        service: hive
        team: srep
    - alert: SelectorSyncsetApplyFailures - integration - {{{shard_name}}}
      annotations:
        message: "The SelectorSyncset {{ $labels.name }} has more than 1 unapplied instances for 15 minutes"
        runbook: "https://github.com/openshift/ops-sop/tree/master/v4/alerts/SelectorSyncsetApplyFailures.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        hive_selectorsyncset_clusters_unapplied_total{job="hive-controllers"} > 0
      for: 15m
      labels:
        service: hive
        severity: medium
        team: srep
    - alert: AWSAccountLimit - integration - {{{shard_name}}}
      annotations:
        message: "AWS Account Operator is about to reach account limit"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/AWSAccountLimit.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/WPRpa7HWz/aws-account-operator?orgId=1&var-datasource={{{grafana_datasource}}}&var-environment=hive-aws-account-operator"
      expr: |
        aws_account_operator_aws_accounts{job="aws-account-operator"} > {{{aws_account_operator_accounts_threshold}}}
      labels:
        severity: high
        service: srep
    - alert: AWSAccountOperator Target Down - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for AWS Account Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="aws-account-operator"} == 1)
      for: 15m
      labels:
        severity: high
        service: srep
    - alert: CertmanOperator Target Down - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for Certman Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="certman-operator"} == 1)
      for: 15m
      labels:
        severity: high
        service: srep
    - alert: DeadMansSnitchOperator Target Down - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for Dead Mans Snitch Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="deadmanssnitch-operator"} == 1)
      for: 15m
      labels:
        severity: high
        service: srep
    - alert: PagerDutyOperator Target Down - integration - {{{shard_name}}}
      annotations:
        message: "No targets found for PagerDuty Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="pagerduty-operator"} == 1)
      for: 15m
      labels:
        severity: high
        service: srep
    - alert: AWS Account Shredder Crash Looping - integration - {{{shard_name}}}
      annotations:
        message: "AWS Account Shredder is crash looping more than 5 times in 15 minutes on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/AwsAccountShredderCrashLooping.md"
      expr: |
        sum(rate(kube_pod_container_status_restarts_total{namespace="aws-account-shredder"}[15m])) > 5
      labels:
        severity: medium
        service: srep

