---
$schema: /app-interface/prometheus-rule-test-1.yml

rule_files:
- /services/hive/hive-production-common.prometheusrules.yaml

evaluation_interval: 1m

tests:

- interval: 1m
  input_series:
  - series: up{job="hive-controllers"}
    values: 1+0x10 0+0x10 # 1 for the first 10 minutes, 0 for the next 10 minutes
  alert_rule_test:
  - eval_time: 3m
    alertname: HiveControllersDown - production - {{{shard_name}}}
    exp_alerts:
  - eval_time: 18m
    alertname: HiveControllersDown - production - {{{shard_name}}}
    exp_alerts:
    - exp_labels:
        severity: critical
        team: appsre
        service: hive
      exp_annotations:
        message: "No targets found for hive-controllers in namespace . hive-controllers is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveControllerDown.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"

- interval: 1m
  input_series:
  - series: up{job="openshift-customer-monitoring/hive-operator"}
    values: 1+0x5 0+0x15 # 1 for the first 5 minutes, 0 for the next 15 minutes
  alert_rule_test:
  - eval_time: 3m
    alertname: HiveOperatorDown - production - {{{shard_name}}}
    exp_alerts:
  - eval_time: 18m
    alertname: HiveOperatorDown - production - {{{shard_name}}}
    exp_alerts:
    - exp_labels:
        severity: critical
        team: appsre
        service: hive
      exp_annotations:
        message: "No targets found for hive-operator in namespace . hive-operator is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveOperatorDown.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"


- interval: 1m
  input_series:
  - series: hive_hiveconfig_conditions{condition="Ready", job="openshift-customer-monitoring/hive-operator", reason="ErrorDeployingHive"}
    values: 1+0x10 0+0x10 # 1 for the first 10 minutes, 0 for the next 10 minutes
  alert_rule_test:
  - eval_time: 3m
    alertname: HiveDeploymentFailed - production - {{{shard_name}}}
    exp_alerts:
  - eval_time: 18m
    alertname: HiveDeploymentFailed - production - {{{shard_name}}}
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: appsre
        job: openshift-customer-monitoring/hive-operator
        condition: Ready
        reason: ErrorDeployingHive
      exp_annotations:
        message: "hive deployment has failed. Condition/Reason: Ready / ErrorDeployingHive."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveDeploymentFailed.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"

# test that a known-by-OCM provisioning failure (i.e. has a well-known code) doesn't alert ever
- interval: 1m
  input_series:
  - series: hive_cluster_deployment_provision_underway_seconds{job="hive-controllers", cluster_type="managed", cluster_deployment="busted-cluster", exported_namespace="uhc-production-something", condition="ProvisionFailed", reason="S3BucketsLimitExceeded"}
    values: '0+60x240' # install starts at t0, increases 600 seconds every interval (10 mins), should be alerted after 120m
  - series: up{job="hive-controllers"}
    values: '1+0x240'
  alert_rule_test:
  - eval_time: 30m
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
  - eval_time: 140m # even after 2 hours, we should NOT get alerted - S3BucketsLimitExceeded is a known problem and OCM describes it to the user well, don't bother SREP for that
    alertname: ClusterProvisioningDelay - production
    exp_alerts:

# test that a newly-known-by-OCM provisioning failure that hasn't yet been added to alertmanager DOES alert
- interval: 1m
  input_series:
  - series: hive_cluster_deployment_provision_underway_seconds{job="hive-controllers", cluster_type="managed", cluster_deployment="busted-cluster", exported_namespace="uhc-production-something", condition="ProvisionFailed", reason="SomeNewReasonThatsNotUnknown"}
    values: '0+60x180 0+0x30 12600+60x60' # install starts at t0, increases 600 seconds every interval (10 mins), should be alerted after 120m
  - series: up{job="hive-controllers"}
    values: '1+0x180 0+0x30 1+0x180'
  alert_rule_test:
  - eval_time: 30m
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
  - eval_time: 140m # after 2 hours, we should get alerted - Unknown reason requires SREP response
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: srep
        job: hive-controllers
        cluster_deployment: busted-cluster
        exported_namespace: uhc-production-something
        condition: ProvisionFailed
        reason: SomeNewReasonThatsNotUnknown
      exp_annotations:
        message: "cluster busted-cluster in namespace uhc-production-something provisioning taking over 2 hours. Condition/Reason: ProvisionFailed / SomeNewReasonThatsNotUnknown. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
  - eval_time: 195m # pod producing metric samples is down - Alert is not dismissed
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: srep
        job: hive-controllers
        cluster_deployment: busted-cluster
        exported_namespace: uhc-production-something
        condition: ProvisionFailed
        reason: SomeNewReasonThatsNotUnknown
      exp_annotations:
        message: "cluster busted-cluster in namespace uhc-production-something provisioning taking over 2 hours. Condition/Reason: ProvisionFailed / SomeNewReasonThatsNotUnknown. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
  - eval_time: 290m # alert is dismissed
    alertname: ClusterProvisioningDelay - production
    exp_alerts:

# test that an Unknown-by-OCM provisioning failure (i.e. does NOT have a well-known code) alerts after 2 hours
- interval: 1m
  input_series:
  - series: hive_cluster_deployment_provision_underway_seconds{job="hive-controllers", cluster_type="managed", cluster_deployment="busted-cluster", exported_namespace="uhc-production-something", condition="ProvisionFailed", reason="Unknown"}
    values: '0+60x180 0+0x780 57600+60x60' # install starts at t0, increases 600 seconds every interval (10 mins), should be alerted after 120m
  - series: up{job="hive-controllers"}
    values: '1+0x180 0+0x780 1+0x180'
  alert_rule_test:
  - eval_time: 30m
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
  - eval_time: 140m # after 2 hours, we should get alerted - Unknown reason requires SREP response
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: srep
        job: hive-controllers
        cluster_deployment: busted-cluster
        exported_namespace: uhc-production-something
        condition: ProvisionFailed
        reason: Unknown
      exp_annotations:
        message: "cluster busted-cluster in namespace uhc-production-something provisioning taking over 2 hours. Condition/Reason: ProvisionFailed / Unknown. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
  - eval_time: 890m # pod producing metric samples has been down for less than 12h - Alert is not dismissed
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: srep
        job: hive-controllers
        cluster_deployment: busted-cluster
        exported_namespace: uhc-production-something
        condition: ProvisionFailed
        reason: Unknown
      exp_annotations:
        message: "cluster busted-cluster in namespace uhc-production-something provisioning taking over 2 hours. Condition/Reason: ProvisionFailed / Unknown. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
  - eval_time: 920m # pod producing metric samples has been down for more than 12h - Alert is dismissed
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
  - eval_time: 980m # pod producing metric samples in up again - Alert is re-triggered
    alertname: ClusterProvisioningDelay - production
    exp_alerts:
    - exp_labels:
        severity: critical
        service: hive
        team: srep
        job: hive-controllers
        cluster_deployment: busted-cluster
        exported_namespace: uhc-production-something
        condition: ProvisionFailed
        reason: Unknown
      exp_annotations:
        message: "cluster busted-cluster in namespace uhc-production-something provisioning taking over 2 hours. Condition/Reason: ProvisionFailed / Unknown. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
  - eval_time: 1040m # alert is dismissed
    alertname: ClusterProvisioningDelay - production
    exp_alerts:

# test that a pending account claim alerts after 10 minutes
- interval: 1m
  input_series:
  - series: aws_account_operator_account_claim_pending_duration_seconds_bucket{name="aws-account-operator",le="+Inf"}
    values: 0+0x1 1+0x1 2+0x13
  - series: aws_account_operator_account_claim_pending_duration_seconds_bucket{name="aws-account-operator",le="600"}
    values: 0+0x1 1+0x14
  alert_rule_test:
  - eval_time: 11m
    alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
    exp_alerts:
    - exp_labels:
        severity: critical
        team: srep
        service: srep
      exp_annotations:
        message: "AWS Account Operator AccountClaim pending for over 10 minutes."
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/aws/aws-account-troubleshooting.md"
        dashboard: "https://grafana.app-sre.devshift.net/explore?orgId=1&left=%7B%22datasource%22:%22{{{shard_name}}}-prometheus%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%20or%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D"

- interval: 1m
  input_series:
    - series: aws_account_operator_account_claim_pending_duration_seconds_bucket{name="aws-account-operator",le="+Inf"}
      values: 1+0x10 2+0x10 3+0x10
    - series: aws_account_operator_account_claim_pending_duration_seconds_bucket{name="aws-account-operator",le="600"}
      values: 1+0x10 2+0x20
  alert_rule_test:
    - eval_time: 10m
      alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
      exp_alerts:
    - eval_time: 30m
      alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
      exp_alerts:
        - exp_labels:
            severity: critical
            team: srep
            service: srep
          exp_annotations:
            message: "AWS Account Operator AccountClaim pending for over 10 minutes."
            runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/aws/aws-account-troubleshooting.md"
            dashboard: "https://grafana.app-sre.devshift.net/explore?orgId=1&left=%7B%22datasource%22:%22{{{shard_name}}}-prometheus%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%20or%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D"

# test that a pending ccs account claim alerts after 10 minutes
- interval: 1m
  input_series:
    - series: aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket{name="aws-account-operator",le="+Inf"}
      values: 0+0x1 1+0x1 2+0x13
    - series: aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket{name="aws-account-operator",le="600"}
      values: 0+0x1 1+0x14
  alert_rule_test:
    - eval_time: 11m
      alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
      exp_alerts:
        - exp_labels:
            severity: critical
            team: srep
            service: srep
          exp_annotations:
            message: "AWS Account Operator AccountClaim pending for over 10 minutes."
            runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/aws/aws-account-troubleshooting.md"
            dashboard: "https://grafana.app-sre.devshift.net/explore?orgId=1&left=%7B%22datasource%22:%22{{{shard_name}}}-prometheus%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%20or%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D"

- interval: 1m
  input_series:
    - series: aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket{name="aws-account-operator",le="+Inf"}
      values: 1+0x10 2+0x10 3+0x10
    - series: aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket{name="aws-account-operator",le="600"}
      values: 1+0x10 2+0x20
  alert_rule_test:
    - eval_time: 10m
      alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
      exp_alerts:
    - eval_time: 30m
      alertname: AWSAccountOperator AccountClaim Pending - production - {{{shard_name}}}
      exp_alerts:
        - exp_labels:
            severity: critical
            team: srep
            service: srep
          exp_annotations:
            message: "AWS Account Operator AccountClaim pending for over 10 minutes."
            runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/aws/aws-account-troubleshooting.md"
            dashboard: "https://grafana.app-sre.devshift.net/explore?orgId=1&left=%7B%22datasource%22:%22{{{shard_name}}}-prometheus%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%20or%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22%2BInf%5C%22%7D%5B10m%5D))%20%3E%20sum(rate(aws_account_operator_account_claim_ccs_pending_duration_seconds_bucket%7Bname%3D%5C%22aws-account-operator%5C%22,le%3D%5C%22600%5C%22%7D%5B10m%5D))%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D"