$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: insights-platform-rules
  labels:
    prometheus: app-sre
    role: alert-rules
spec:
  groups:
  - name: insights-sli
    rules:
      # Constants passed in during deploy via app-interface
    - record: service:constant:sli:pcterror
      expr: {{{ ERROR_RATE_THRESHOLD_PCT }}}
    - record: service:constant:sli:pctlatency
      expr: {{{ LATENCY_THRESHOLD_PCT }}}

      # Get total requests, returns for all exported_services
    - record: service:request_count:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_time_count[5m])) > 0

      # Get all 500 errors, returns for all exported_services
    - record: service:status_5xx:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_status{ status="5xx" }[5m]))

      # Get the ratio of 500 / total requests, returns for all exported_services
    - record: service:proportion:status_5xx:rate5m
      expr: service:status_5xx:rate5m / service:request_count:rate5m

      # THRESHOLD is set to 5. Use the ratio to return a true or false for threshold met
      # Averaged over 30 days, we get the % amount of time a the SLI was met
    - record: service:sli:status_5xx:pctl{{{ ERROR_RATE_THRESHOLD_PCT }}}rate5m
      expr: service:proportion:status_5xx:rate5m * 100 < bool {{{ ERROR_RATE_THRESHOLD_PCT }}}

      # Latency Threshold is 2000ms by default, or 2s
      # Get the number of requests that were within the the 2s threshold
    - record: service:latency_le_{{{ LATENCY_THRESHOLD_MS }}}:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_time_bucket{le="{{{ LATENCY_THRESHOLD_MS }}}.0" }[5m]))

      # Get the ratio of compliant (< 2s) requests, and take the inverse to get downtime
    - record: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m
      expr: 1 - (service:latency_le_{{{ LATENCY_THRESHOLD_MS }}}:rate5m / service:request_count:rate5m)

      # Using that downtime ratio, return the % amount of time the SLI was met
    - record: service:sli:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:pctl{{{ LATENCY_THRESHOLD_PCT }}}rate5m
      expr: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m * 100 < bool {{{ LATENCY_THRESHOLD_PCT }}}

    # Dynamic service based metrics
    - record: service:sli:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:pctlrate5m
      expr: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m * 100 < bool on(exported_service) service:constant:sli:pctlatency
    - record: service:sli:status_5xx:pctlrate5m
      expr: service:proportion:status_5xx:rate5m * 100 < bool on(exported_service) service:constant:sli:pcterror

    # Dynamic latency based metrics
    - record: service:proportion:latency_adjusted:pctlrate5m
      expr: service:latency:adjusted:rate5m * 100 < bool on (exported_service) service:constant:sli:pctlatency

  - name: network-check
    rules:
    - record: HttpFailuresLast1Min
      expr: increase(netcheck_http_failures_total[1m])
    - record: GetentFailuresLast1Min
      expr: increase(netcheck_getent_failures_total[1m])
    - alert: HttpFailuresAlert
      expr: HttpFailuresLast1Min > 0
      for: 5m
      labels:
        severity: medium
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/6Y8XkcHGz/network-check?orgId=1"
        message: "insights network-check HTTP check has ongoing failures for the last 5min"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/cloud.redhat.com/network-check.rst
    - alert: GetentFailuresAlert
      expr: GetentFailuresLast1Min > 0
      for: 5m
      labels:
        severity: medium
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/6Y8XkcHGz/network-check?orgId=1"
        message: "insights network-check getent check has ongoing failures for the last 5min"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/cloud.redhat.com/network-check.rst
  - name: lag
    rules:
    - record: LagGauge
      expr: sum(kafka_consumergroup_group_lag{topic!~"__.*"}) by (topic, group)
    - record: LagDelta
      expr: sum(delta(kafka_consumergroup_group_lag{topic!~"__.*"}[5m])) by (topic, group)
    - record: InventoryLag
      expr: 10000
      labels:
        group: inventory-mq
        topic: platform.inventory.host-ingress-p1
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-puptoo
        topic: platform.upload.advisor
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-core-kafka
        topic: platform.inventory.events
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: advisor_results
        topic: platform.engine.results
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-storage-broker
        topic: platform.inventory.events
    - alert: KafkaLagFast
      expr: LagGauge > LAG_THRESHOLD and LagDelta > 1000
      for: 15m
      labels:
        severity: critical
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.stage.devshift.net"
        message: "High Lag for {{ $labels.topic }} : {{ $labels.group }}"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/cloud.redhat.com/kafka-lag.rst
    - alert: KafkaLag
      expr: LagGauge > LAG_THRESHOLD and LagDelta > 0
      for: 1h
      labels:
        severity: critical
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.stage.devshift.net"
        message: "High Lag for {{ $labels.topic }} : {{ $labels.group }}"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/cloud.redhat.com/kafka-lag.rst
# TODO create and add runbook for the Clowder alerts
  - name: clowder alerts
    rules:
    - alert: ClowderReconciliationRate
      expr: rate(controller_runtime_reconcile_total[5m]) > 10
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: https://grafana.stage.devshift.net/d/VTSfF_0Gk/clowder-metrics?viewPanel=6&orgId=1&var-datasource=crcs02ue1-prometheus
        message: "Clowder Reconciliation Rate too high"
    - alert: ClowderReconciliationErrorRate
      expr: sum(rate(controller_runtime_reconcile_total{controller=~"clowdenvironment|clowdapp",result="error"}[5m])) by (controller) > 0
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: https://grafana.stage.devshift.net/d/VTSfF_0Gk/clowder-metrics?orgId=1&var-datasource=crcs02ue1-prometheus
        message: "Clowder Reconciliation Error Rate too high"
    - alert: ClowdAppsCount
      expr: sum(clowd_app_managed_apps) == 0
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: 
        message: "ClowdApp not running"
    - alert: ClowderPodCount
      expr: count by (pod) (clowd_app_managed_apps) == 0
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: 
        message: "Clowder pod not running for 5minutes"
    - alert: ClowderContainerRestartCount
      expr: sum(increase(kube_pod_container_status_restarts_total{namespace="clowder", container="manager"}[5m])) > 10
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: 
        message: "Too many Clowder pod restarts"
    - alert: ClowderWorkQueueDepth
      expr: sum(workqueue_depth{service="clowder-controller-manager"}) by (name) > 20
      for: 5m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: 
        message: "Clowder Work Queue depth too deep "
