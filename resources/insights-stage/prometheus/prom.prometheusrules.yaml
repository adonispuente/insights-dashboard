$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: insights-platform-rules
  labels:
    prometheus: app-sre
    role: alert-rules
spec:
  groups:
  - name: insights-sli
    rules:
      # Constants passed in during deploy via app-interface
    - record: service:constant:sli:pcterror
      expr: {{{ ERROR_RATE_THRESHOLD_PCT }}}
    - record: service:constant:sli:pctlatency
      expr: {{{ LATENCY_THRESHOLD_PCT }}}

      # Get total requests, returns for all exported_services
    - record: service:request_count:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_time_count[5m])) > 0

      # Get all 500 errors, returns for all exported_services
    - record: service:status_5xx:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_status{ status="5xx" }[5m]))

      # Get the ratio of 500 / total requests, returns for all exported_services
    - record: service:proportion:status_5xx:rate5m
      expr: service:status_5xx:rate5m / service:request_count:rate5m

      # THRESHOLD is set to 5. Use the ratio to return a true or false for threshold met
      # Averaged over 30 days, we get the % amount of time a the SLI was met
    - record: service:sli:status_5xx:pctl{{{ ERROR_RATE_THRESHOLD_PCT }}}rate5m
      expr: service:proportion:status_5xx:rate5m * 100 < bool {{{ ERROR_RATE_THRESHOLD_PCT }}}

      # Latency Threshold is 2000ms by default, or 2s
      # Get the number of requests that were within the the 2s threshold
    - record: service:latency_le_{{{ LATENCY_THRESHOLD_MS }}}:rate5m
      expr: sum by(exported_service, environment) (rate(api_3scale_gateway_api_time_bucket{le="{{{ LATENCY_THRESHOLD_MS }}}.0" }[5m]))

      # Get the ratio of compliant (< 2s) requests, and take the inverse to get downtime
    - record: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m
      expr: 1 - (service:latency_le_{{{ LATENCY_THRESHOLD_MS }}}:rate5m / service:request_count:rate5m)

      # Using that downtime ratio, return the % amount of time the SLI was met
    - record: service:sli:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:pctl{{{ LATENCY_THRESHOLD_PCT }}}rate5m
      expr: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m * 100 < bool {{{ LATENCY_THRESHOLD_PCT }}}

    # Dynamic service based metrics
    - record: service:sli:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:pctlrate5m
      expr: service:proportion:latency_gt_{{{ LATENCY_THRESHOLD_MS }}}:rate5m * 100 < bool on(exported_service) service:constant:sli:pctlatency
    - record: service:sli:status_5xx:pctlrate5m
      expr: service:proportion:status_5xx:rate5m * 100 < bool on(exported_service) service:constant:sli:pcterror

    # Dynamic latency based metrics
    - record: service:proportion:latency_adjusted:pctlrate5m
      expr: service:latency:adjusted:rate5m * 100 < bool on (exported_service) service:constant:sli:pctlatency

  - name: network-check
    rules:
    - record: HttpFailuresLast1Min
      expr: increase(netcheck_http_failures_total[1m])
    - record: GetentFailuresLast1Min
      expr: increase(netcheck_getent_failures_total[1m])
    - alert: HttpFailuresAlert
      expr: HttpFailuresLast1Min > 0
      for: 5m
      labels:
        severity: medium
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/6Y8XkcHGz/network-check?orgId=1"
        message: "insights network-check HTTP check has ongoing failures for the last 5min"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/console.redhat.com/network-check.rst
    - alert: GetentFailuresAlert
      expr: GetentFailuresLast1Min > 0
      for: 5m
      labels:
        severity: medium
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/6Y8XkcHGz/network-check?orgId=1"
        message: "insights network-check getent check has ongoing failures for the last 5min"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/console.redhat.com/network-check.rst
  - name: lag
    rules:
    - record: LagGauge
      expr: sum(kafka_consumergroup_group_lag{topic!~"__.*"}) by (topic, group)
    - record: LagDelta
      expr: sum(delta(kafka_consumergroup_group_lag{topic!~"__.*"}[5m])) by (topic, group)
    - record: InventoryLag
      expr: 10000
      labels:
        group: inventory-mq
        topic: platform.inventory.host-ingress-p1
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-puptoo
        topic: platform.upload.advisor
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-core-kafka
        topic: platform.inventory.events
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: advisor_results
        topic: platform.engine.results
    - record: LAG_THRESHOLD
      expr: 10000
      labels:
        group: insights-storage-broker
        topic: platform.inventory.events
    - record: is_us_summer_time
      expr: |
        (vector(1) and (month() > 3 and month() < 10))
        or
        (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 10) and absent((day_of_month() >= 10) and (day_of_week() == 0)))
        or
        (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 10) and absent((day_of_month() >= 10) and (day_of_week() == 0)))
        or
        (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 10) and (day_of_week() == 0)))
        or
        vector(0)
    - record: us_time
      expr: time() - 14400 * is_us_summer_time
    - record: us_hour
      expr: hour(us_time)
    - alert: KafkaLagFast
      expr: LagGauge > LAG_THRESHOLD and LagDelta > 1000 and (us_hour >= 9 and us_hour <= 22)
      for: 15m
      labels:
        severity: critical
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.stage.devshift.net"
        message: "High Lag for {{ $labels.topic }} : {{ $labels.group }}"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/console.redhat.com/kafka-lag.rst
    - alert: KafkaLag
      expr: LagGauge > LAG_THRESHOLD and LagDelta > 0 and (us_hour >= 9 and us_hour <= 22)
      for: 1h
      labels:
        severity: critical
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://grafana.stage.devshift.net"
        message: "High Lag for {{ $labels.topic }} : {{ $labels.group }}"
        runbook: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/console.redhat.com/kafka-lag.rst
    - alert: KafkaLagExporterDown
      expr: up{namespace="platform-mq-stage", container="kafka-lag-exporter-stage"} != 1 
      for: 15m
      labels:
        severity: high
        service: insights
        env: {{{ env }}}
      annotations:
        dashboard: "https://console-openshift-console.apps.crcs02ue1.urby.p1.openshiftapps.com/k8s/ns/platform-mq-stage/pods"
        message: "Kafka Lag Exporter pod down for the last 15minutes"
# BlackBox Monitor alert
  - name: DevProd BlackBox Monitor
    rules:
    - alert: BlackBox Monitor offline
      expr: sum(kube_replicationcontroller_status_ready_replicas{namespace="blackbox-monitor-stage"}) < 1
      for: 15m
      labels:
        severity: medium
        service: insights
        env: {{{ env }}}
      annotations:
        message: "BlackBox Monitor stage is offline"
        runbook: "https://clouddot.pages.redhat.com/docs/dev/developer-references/sop/blackbox-monitor.html"
