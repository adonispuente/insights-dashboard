apiVersion: argoproj.io/v1alpha1
kind: ArgoCD
metadata:
  name: argocd
spec:
  server:
    autoscale:
      enabled: false
    grpc:
      ingress:
        enabled: false
    ingress:
      enabled: false
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 125m
        memory: 128Mi
    route:
      enabled: true
      tls:
        termination: reencrypt
    service:
      type: ''
  grafana:
    enabled: false
    ingress:
      enabled: false
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
    route:
      enabled: false
  notifications:
    enabled: false
  prometheus:
    enabled: false
    ingress:
      enabled: false
    route:
      enabled: false
  initialSSHKnownHosts: {}
  sso:
    dex:
      openShiftOAuth: true
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 250m
          memory: 128Mi
    provider: dex
  applicationSet:
    resources:
      limits:
        cpu: '2'
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi
    webhookServer:
      ingress:
        enabled: false
      route:
        enabled: false
  rbac:
    policy: |
      p, role:developer, applications, sync, default/*, allow
      p, role:developer, applications, get, default/*, allow
      p, role:developer, logs, get, default/*, allow

      g, argocd-developers, role:developer

    scopes: '[groups]'
  repo:
    resources:
      limits:
        cpu: '1'
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 256Mi
  resourceExclusions: |
    - apiGroups:
      - tekton.dev
      clusters:
      - '*'
      kinds:
      - TaskRun
      - PipelineRun
  ha:
    enabled: false
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  tls:
    ca: {}
  redis:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  controller:
    processors: {}
    resources:
      limits:
        cpu: 2
        memory: 4Gi
      requests:
        cpu: 2
        memory: 4Gi
    sharding: {}
  resource.customizations: |
    logging.openshift.io/ClusterLogForwarder:
      health.lua: |
        local obj = resource
        local hs = {
          status = "Progressing",
          message = "Waiting for pipeline readiness status"
        }
        local allPipelines = {}
        local notReadyPipelines = {}

        local pipelines = obj.status and obj.status.pipelines or {}
        for _, pipeline in ipairs(pipelines) do
            local conditions = pipeline.conditions or { { type = "Ready", status = "False", message = "The conditions field is missing for " .. pipeline.name } }
            for _, condition in ipairs(conditions) do
                if condition.type == "Ready" then
                    hs.message = condition.message
                    hs.status = condition.status == "False" and "Degraded" or "Healthy"
                    table.insert(allPipelines, pipeline.name)
                    if hs.status == "Degraded" then
                        table.insert(notReadyPipelines, pipeline.name)
                    end
                end
            end
        end

        hs.status = #notReadyPipelines == 0 and "Healthy" or "Degraded"
        hs.message = #notReadyPipelines == 0 and "All pipelines are ready" or "Not ready pipelines: " .. table.concat(notReadyPipelines, ", ")

        return hs
